---
title: "Final Project Main"
author: "James Spalding"
date: "2023-05-02"
output: html_document
---

## Our initial goal was to see if songs became more repetitive over time. To do this, we found a resource to show the top 20 songs of each year. After that, we created links for genius.com for each song on this list. After generating these links, we can read them and pull the lyrics from each link and throw them into another dataframe. The data gathering was by far the most difficult and time-consuming part of this project and the analysis was very straightforward.

```{r setup, include=FALSE}
#All libraries required
library(rvest)
library(tidyverse)
library(wordcloud2)
library(tidytext)
library(tidyr)
```

## Main backend work. Read all data from 1958-2022 on the top 50 singles. It then grabs the top 20 and puts their names, year, and artist into a dataframe. It then uses the artist name and song title to generate links to geenius.com to grab lyrics. This process ended up taking over an hour to run, so we ran it once and put it into a CSV file so that we wouldn't have to run it every time. -James and Ben

```{r}
#James Work: Main setup and data gathering link generation. Commented all print statements to clean up the output.

#Used Ben's code to make an initial table to bind all future years to
url1958 = "https://en.wikipedia.org/wiki/Billboard_year-end_top_50_singles_of_1958"
read1958 = read_html(url1958)
table_1958 = read1958 %>% html_table
table_1958 = data.frame(table_1958[[1]])
year = rep(1958, each = 51)
new_table = cbind(table_1958, year)
table_1958 = setNames(object = new_table, c("No.", "Title", "Artist", "Year")) %>%
  relocate("Year", .before = "No." )

  #Wrote this code to Separate double songs. Wil be used throughout data gathering
  if (any(grepl("/", table_1958$Title))) {
    table_1958 <- table_1958 %>%
      separate_rows(Title, sep = "/")
  }

#Creates topSongs table
topSongs = table_1958[1:24,]


for(yearVar in 1959:2011){
  #Using the iterated yearVar variable to get the url for any given year
  urlTemp = "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_"
  yearString = as.character(yearVar)
  url = paste(urlTemp, yearString, sep = "")
  readUrl = read_html(url)
  
  tableTemp = readUrl %>% html_table 
  tableTemp = data.frame(tableTemp[[1]])
  
  #Need an if statement, because the song number is either referenced as X. or No. depending on the year
  if("X." %in% colnames(tableTemp)){
    tableTemp = tableTemp %>% filter(as.numeric(X.) <= 20)
  }else{
    tableTemp = tableTemp %>% filter(as.numeric(No.) <= 20)
  }
  yearCol = rep(yearVar, each = 20)
  tableTemp.2 = cbind(tableTemp, yearCol[1:10]) 
  tableTemp = setNames(object = tableTemp.2, c("No.", "Title", "Artist", "Year")) %>%
    relocate("Year", .before = "No." )
  

  #Separates double songs
  if (any(grepl("/", tableTemp$Title))) {
    tableTemp <- tableTemp %>%
      separate_rows(Title, sep = "/")
  }
  
  topSongs = rbind(topSongs, tableTemp)
  #print(yearVar)
}

#2012 and 2013 use a different table for some reason, so they must be done individually.

for(yearVar in 2012:2013){
  #Using the iterated yearVar variable to get the url for any given year
  urlTemp = "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_"
  yearString = as.character(yearVar)
  url = paste(urlTemp, yearString, sep = "")
  readUrl = read_html(url)
  
  tableTemp = readUrl %>% html_table 
  tableTemp = data.frame(tableTemp[[2]])
  tableTemp = tableTemp %>% filter(as.numeric(No.) <= 20)
  yearCol = rep(yearVar, each = 20)
  tableTemp.2 = cbind(tableTemp, yearCol[1:10]) 
  tableTemp = setNames(object = tableTemp.2, c("No.", "Title", "Artist", "Year")) %>%
    relocate("Year", .before = "No." )
  

  #Separates double songs
  if (any(grepl("/", tableTemp$Title))) {
    tableTemp <- tableTemp %>%
      separate_rows(Title, sep = "/")
  }

  topSongs = rbind(topSongs, tableTemp)
  #print(yearVar)
}

#Using the original code to get the rest of the years.

for(yearVar in 2014:2022){
  #Using the iterated yearVar variable to get the url for any given year
  urlTemp = "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_"
  yearString = as.character(yearVar)
  url = paste(urlTemp, yearString, sep = "")
  readUrl = read_html(url)
  
  tableTemp = readUrl %>% html_table 
  tableTemp = data.frame(tableTemp[[1]])
  
  #Need an if statement, because the song number is either referenced as X. or No. depending on the year
  if("X." %in% colnames(tableTemp)){
    tableTemp = tableTemp %>% filter(as.numeric(X.) <= 20)
  }else{
    tableTemp = tableTemp %>% filter(as.numeric(No.) <= 20)
  }
  yearCol = rep(yearVar, each = 20)
  tableTemp.2 = cbind(tableTemp, yearCol[1:10]) 
  tableTemp = setNames(object = tableTemp.2, c("No.", "Title", "Artist", "Year")) %>%
    relocate("Year", .before = "No." )

  #Separates double songs
  if (any(grepl("/", tableTemp$Title))) {
    tableTemp <- tableTemp %>%
      separate_rows(Title, sep = "/")
  }

  topSongs = rbind(topSongs, tableTemp)
  #print(yearVar)
}

###################

#Generate URLs

URLVector = c() #Can use this vector (once filled) in later functions.

for(i in 1:1300){
  #Grab and format song names
  songnameTemp = topSongs[i,3]
  songnameTemp2 = gsub("[[:punct:]]", "", songnameTemp)
  songnameFormat = gsub(" ", "-", songnameTemp2) %>%
    tolower()
  
  #Some songs are split into 2; need an if statement.
  if(grepl("--", songnameFormat)){
    songnameFormat = strsplit(songnameFormat, "--")[[1]] %>%
      tolower()
    #print(songnameFormat)
  } else{
    #print(songnameFormat)
  }
  
  #Grab and format artist name
  artistnameTemp = topSongs[i,4]
  artistnameTemp2 = gsub("[[:punct:]]", "", artistnameTemp)
  artistnameTemp3 = gsub(" ", "-", artistnameTemp2)
  artistnameFormat = paste(artistnameTemp3, "-", sep = "")
  #print(artistnameFormat)
  
  #Need to remove featured artists. If there is a featured artist, it will drop their name. Otherwise, it will print song as-is (or multiple if applicable)
  if(grepl("-featuring", artistnameFormat)){
    artistnameFormat = strsplit(artistnameFormat, "featuring")[[1]]
    #print(songnameFormat[1])
    geniusURLTemp = "https://genius.com/"
    geniusURL = paste(geniusURLTemp, artistnameFormat, songnameFormat, "-lyrics", sep = "")
    #print(geniusURL[1])
    URLVector = append(URLVector, geniusURL[1])
  } else{
    geniusURLTemp = "https://genius.com/"
    geniusURL = paste(geniusURLTemp, artistnameFormat, songnameFormat, "-lyrics", sep = "")
    #print(geniusURL)
    URLVector = append(URLVector, geniusURL)
  }
}

###################

#Grab and format lyrics from links
#This section is unnecissary in the final project. All of its output is stored into Test_file.csv so that it doesn't need to be run each time. Running the entire function takes roughly an hour and generates lyrics from 1300 songs.

#Lyric function from ben
# get_lyrics <- function(urls) {
#   lyrics_vec <- vector(mode = "character", length(urls))
#   
#   for (i in seq_along(urls)) {
#     tryCatch({
#       html <- read_html(urls[i])
#       lyrics_node <- html_node(html, ".Dzxov , .jAzSMw , i") #.Dzxov
#       lyrics_text <- html_text(lyrics_node)
#       
#       #Formatting the lyrics correctly. These can PROBABLY be written into one line, but it sounds tedious so I didn't bother.
#       lyricsFormatTemp =  gsub("\\[.*?\\]", "", lyrics_text) #Removes square brackets
#       lyricsFormatTemp2 = gsub("[']", "", lyricsFormatTemp) #Removes apostrophes
#       lyricsFormat = gsub("(?<=[a-z])(?=[A-Z])", " ", lyricsFormatTemp2, perl=TRUE) #Adds spaces before capitals
#       
#       lyrics_vec[i] <- lyricsFormat
#       print(URLVector[i])
#     }, error = function(e) {
#       lyrics_vec[i] <- "Not Found"
#     })
#   }
#   return(lyrics_vec)
# }

#Can grab all links by indexing URLVector[]
#DISCLAIMER: Will take ~1 hour to run full URLVector
#testlyric = get_lyrics(URLVector[1:10])


#Put ben's code into a function to test various values.
# wordcount = function(x){
#   words = strsplit(tolower(x), "\\W+")
#   word_counts = table(words)
#   word_counts[order(-word_counts)]
# }

###################

#Formatting CSV: testFile.csv
testFile = read_csv("Test_file.csv")

#This is the final songs + lyrics dataframe out of sync
songsWithLyrics = cbind(topSongs, testFile) %>%
  setNames(c("Year", "No.", "Title", "Artist", "Lyrics"))

#Made by ben
word_count_mean <- function(strings) {
  # replace blank or NA values with 0
  strings[is.na(strings) | strings == ""] <- NA
  
  # split strings into individual words for each index
  words_list <- strsplit(strings, " ")
  
  # count the number of times each word appears in each index
  counts_list <- lapply(words_list, table)
  
  # calculate the mean count of each word for each index
  mean_counts_list <- lapply(counts_list, function(counts) {
    # replace NA values with 0
    counts[is.na(counts)] <- 0
    mean(counts)
  })
  
  # combine the mean counts for each index into a single vector
  mean_counts <- unlist(mean_counts_list)
  
  return(mean_counts)
}

####################
```

## Build a line plot and dot plot overlaid using ggplot2 to answer our initial question of whether or not repitition changed over time. -James

```{r}
#Building ggplot to see if there is any sort of trend in repetition in songs by year -James
wordMeans = word_count_mean(testFile$x)

topSongsMeans = cbind(topSongs, wordMeans)

#Need to build new table with year and average mean.

yearMeans = select(topSongsMeans, "Year", "wordMeans") 
  
for(i in 1:1318){
  if(is.nan(yearMeans$wordMeans[i])){
    yearMeans$wordMeans[i] = 1.25 #Estimated mean word count
  }
}

#Creates a new df with the average repetition by year.
year_average = aggregate(wordMeans ~ Year, data = yearMeans, FUN = mean)

plot = ggplot(year_average, aes(Year, wordMeans))

plot+ geom_point(data = yearMeans, color = "red", alpha = .3)+
  geom_line(size = 1.5) +ylim(c(1.25,4)) +ylab("Mean Repitition")

```

## As a way to visualize which words were repeated most, we created a word cloud. -Ben

```{r}
#Word cloud made by ben
create_wordcloud <- function(strings) {
  # Convert vector of strings to a dataframe
  df <- data.frame(text = strings)
  
  # Tokenize the text into individual words
  df_tokens <- df %>%
    unnest_tokens(word, text)
  
  # Remove stop words (common words like "the", "and", etc.)
  df_tokens <- df_tokens %>%
    anti_join(stop_words)
  
  # Remove punctuation and convert to lowercase
  df_tokens$word <- tolower(gsub("[[:punct:]]", "", df_tokens$word))
  
  # Count the frequency of each word
  word_counts <- df_tokens %>%
    count(word, sort = TRUE)
  
  # Create a word cloud
  wordcloud2(data = word_counts, size = 0.5)
}
create_wordcloud(testFile$x)
```

## Counts the numbers of occurances and used this to build word clouds for 20 year increments. This shows how top words have changed over time. -Sam 

```{R}
library(tidytext)
library(tidyverse)
library(tidyr)

get_lyrics <- function(urls) {
  lyrics_vec <- vector(mode = "character", length(urls))
  
  for (i in seq_along(urls)) {
    tryCatch({
      html <- read_html(urls[i])
      lyrics_node <- html_node(html, ".Dzxov , .jAzSMw , i") #.Dzxov
      lyrics_text <- html_text(lyrics_node)
      
      #Formatting the lyrics correctly. These can PROBABLY be written into one line, but it sounds tedious so I didn't bother.
      lyricsFormatTemp =  gsub("\\[.*?\\]", "", lyrics_text) #Removes square brackets
      lyricsFormatTemp2 = gsub("[']", "", lyricsFormatTemp) #Removes apostrophes
      lyricsFormat = gsub("(?<=[a-z])(?=[A-Z])", " ", lyricsFormatTemp2, perl=TRUE) #Adds spaces before capitals
      
      lyrics_vec[i] <- lyricsFormat
      print(URLVector[i])
    }, error = function(e) {
      lyrics_vec[i] <- "Not Found"
    })
  }
  return(lyrics_vec)
}

#Can grab all links by indexing URLVector[]
#DISCLAIMER: Will take ~1 hour to run full URLVector
#testlyric = get_lyrics(URLVector[1:length(URLVector)])

# Read the dataframe from csv file
write.csv(testlyric, "my_file.csv", row.names = FALSE)
data <- read.table("my_file.csv", header = TRUE, sep = ",", quote = "\"", stringsAsFactors = FALSE)
# Convert dataframe to tidy format
tidy_data <- data %>%
  gather(column_name, text) %>%
  select(-column_name)

# Tokenize the text into individual words
tidy_data_tokens <- tidy_data %>%
  unnest_tokens(word, text)

# Count the frequency of each word
word_freq <- tidy_data_tokens %>%
  count(word, sort = TRUE)

# Print the frequency of each word
print(word_freq)
```
